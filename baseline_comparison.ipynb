{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590fc871",
   "metadata": {},
   "source": [
    "<h2> compare two model result </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b59af1",
   "metadata": {},
   "source": [
    "<h3>1. Load both models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ff309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH1 = \"\"\n",
    "MODEL_PATH2 = \"\"\n",
    "N_CLASSES = 12\n",
    "REAL_TEST_DIR = \"\"\n",
    "BATCH_SIZE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca80527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model 1\n",
    "model1 = models.resnet18(pretrained=False)\n",
    "model1.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "model1.load_state_dict(torch.load(MODEL_PATH1, map_location=DEVICE))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model 1\n",
    "model2 = models.resnet18(pretrained=False)\n",
    "model2.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "model2.load_state_dict(torch.load(MODEL_PATH2, map_location=DEVICE))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbed22",
   "metadata": {},
   "source": [
    "<h3> 2. Create Accuracy Testing  and t-SNE for both model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df1bc6",
   "metadata": {},
   "source": [
    "# data Loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(REAL_TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "classes = test_dataset.classes\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using model 1\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_feature = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model1(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "print(f'Baseline Model Accuracy on Real Test Set: {accuracy:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using model 2\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model2(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "print(f'Baseline Model Accuracy on Real Test Set: {accuracy:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758aaf5a",
   "metadata": {},
   "source": [
    "<h3> Compare the feature between model </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

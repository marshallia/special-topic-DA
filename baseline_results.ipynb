{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Evaluation on Real Images\n",
    "\n",
    "This notebook evaluates the baseline ResNet18 model trained on simulated images on the real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "REAL_TEST_DIR = 'data/real/test'  # Path to real test images\n",
    "MODEL_PATH = 'model/baseline_resnet18.pth'\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Transforms & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(REAL_TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "classes = test_dataset.classes\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate on Real Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "print(f'Baseline Model Accuracy on Real Test Set: {accuracy:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix on Real Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Show Sample Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "misclassified_idx = np.where(all_preds != all_labels)[0]\n",
    "print(f'Total Misclassified Samples: {len(misclassified_idx)}')\n",
    "\n",
    "if len(misclassified_idx) > 0:\n",
    "    sample_idx = random.sample(list(misclassified_idx), min(12, len(misclassified_idx)))\n",
    "    images = []\n",
    "    titles = []\n",
    "    for idx in sample_idx:\n",
    "        path, _ = test_dataset.samples[idx]\n",
    "        img = transforms.ToTensor()(transforms.Resize((224,224))(transforms.Image.open(path).convert('RGB')))\n",
    "        images.append(img)\n",
    "        titles.append(f'True: {classes[all_labels[idx]]}\\nPred: {classes[all_preds[idx]]}')\n",
    "    \n",
    "    grid_img = make_grid(images, nrow=4)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(np.transpose(grid_img.numpy(), (1,2,0)))\n",
    "    plt.axis('off')\n",
    "    plt.title('Sample Misclassified Images')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
